{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7HpQaDByMSO/zhiu4OPy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benedictakel/Sentiment-Analysis-with-LSTM-IMDB-Dataset-/blob/main/Sentiment_Analysis_with_LSTM_(IMDB_Dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak_0YQTZ7fne"
      },
      "outputs": [],
      "source": [
        "pip install torch torchvision torchtext matplotlib scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = IMDB(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for label, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\", \"<pad>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ],
      "metadata": {
        "id": "rBCErGsS8FcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def preprocess(text):\n",
        "    return vocab(tokenizer(text))\n",
        "\n",
        "def collate_batch(batch):\n",
        "    labels, texts = [], []\n",
        "    for label, text in batch:\n",
        "        labels.append(1 if label == 'pos' else 0)\n",
        "        processed = torch.tensor(preprocess(text), dtype=torch.long)\n",
        "        texts.append(processed)\n",
        "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return padded_texts, torch.tensor(labels)\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "train_dataloader = DataLoader(list(train_iter), batch_size=16, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "test_iter = IMDB(split='test')\n",
        "test_dataloader = DataLoader(list(test_iter), batch_size=16, shuffle=False, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "pxc826lx8Jn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        return self.fc(self.dropout(hidden[-1]))\n"
      ],
      "metadata": {
        "id": "dsKv95fJ8P1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(len(vocab), 128, 256, 1).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "def train(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for text, label in dataloader:\n",
        "        text, label = text.to(device), label.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text).squeeze(1)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "for epoch in range(5):\n",
        "    loss = train(model, train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "gHh-v3ur8UxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, truths = [], []\n",
        "    with torch.no_grad():\n",
        "        for text, label in dataloader:\n",
        "            text, label = text.to(device), label.to(device).float()\n",
        "            output = model(text).squeeze(1)\n",
        "            pred = torch.round(torch.sigmoid(output))\n",
        "            preds += pred.cpu().numpy().tolist()\n",
        "            truths += label.cpu().numpy().tolist()\n",
        "    acc = accuracy_score(truths, preds)\n",
        "    prec = precision_score(truths, preds)\n",
        "    rec = recall_score(truths, preds)\n",
        "    print(f\"Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
        "\n",
        "evaluate(model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "3rzITpTI8Z09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab[\"<pad>\"])\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
        "        context = torch.sum(attn_weights * lstm_out, dim=1)\n",
        "        return self.fc(self.dropout(context))\n"
      ],
      "metadata": {
        "id": "w8hcV--O8fbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}